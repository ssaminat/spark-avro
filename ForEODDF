val rdd=sc.textFile("hdfs://nnscbhaasdev/apps/hive/warehouse/scudee_dev_ops_ptest.db/gps_all_tl_eod_marker/*")

val rdd2=rdd.map(x=>x.split("\u0001"))
val rdd2=rdd.map(x=>x.split("\u0001")(4))
val rdd3=rdd2.filter(x=>x(7).equals("2017-02-05"))

val rdd2=rdd.map(x=> if(x.split("\u0001")(6).equals("2017-02-05 23:00:00"))

--------------------------------------------

val df = sqlContext.sql("select * from scudee_dev_ops_ptest.gps_all_tl_eod_marker")
val df1=df.where("edmp_partitiondate='2017-02-05'").select("eod_date","next_eod_date")

(|           eod_date|      next_eod_date|
+-------------------+-------------------+
|2017-02-05 23:23:00|2017-02-06 23:23:00|)


val df2=df.orderBy(desc("edmp_partitiondate")).select("edmp_partitiondate").distinct.take(2)
(or)
val df2=df.sort(df.col("edmp_partitiondate").desc).select("edmp_partitiondate").distinct().take(2);

( Array[org.apache.spark.sql.Row] = Array([2017-02-05], [2017-02-04]))


val df3=df.groupBy("edmp_partitiondate").agg(max("c_journaltime").as("c_journaltime")).orderBy("c_journaltime").show

var eod1=df2(0).toString.replace("[", "").replace("]", "")
var eod2=df2(1).toString.replace("[", "").replace("]", "")
if(df2.length==1)
{
var eod1=df2(0).toString.replace("[", "").replace("]", "")
val df3=df.groupBy("edmp_partitiondate").agg(max("c_journaltime").as("c_journaltime")).orderBy("c_journaltime")
val df4=df3.where(col("edmp_partitiondate").equalTo(eod1)).show
}
--val df3=df.groupBy("edmp_partitiondate").agg(org.apache.spark.sql.functions.max(eodMarkerDF.col("c_journaltime")).as("c_journaltime")).orderBy("c_journaltime");

else
{
val df3=df.groupBy("edmp_partitiondate").agg(max("c_journaltime").as("c_journaltime")).orderBy("c_journaltime")
val df4=df3.where(col("edmp_partitiondate").equalTo(eod1)||col("edmp_partitiondate").equalTo(eod2)).show
}
