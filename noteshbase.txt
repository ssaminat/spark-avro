val hBaseRDD = sc.newAPIHadoopRDD(hbaseconf, classOf[TableInputFormat], classOf[ImmutableBytesWritable], classOf[Result])


val hBaseRDD = sc.newAPIHadoopRDD(hbaseConfig, classOf[TableInputFormat],

      classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable],

      classOf[org.apache.hadoop.hbase.client.Result])
	  
	  
	  
import org.apache.hadoop.hbase.HBaseConfiguration
import org.apache.hadoop.hbase.client.Put
import org.apache.hadoop.hbase.client.Result
import org.apache.hadoop.hbase.io.ImmutableBytesWritable
import org.apache.hadoop.hbase.mapred.TableOutputFormat
import org.apache.hadoop.hbase.mapreduce.TableInputFormat
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.mapred.JobConf
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.rdd.PairRDDFunctions
import org.apache.spark.rdd.RDD.rddToPairRDDFunctions
import org.apache.spark.util.StatCounter
import org.apache.spark._
import org.apache.spark.rdd.NewHadoopRDD
import org.apache.hadoop.hbase.{HBaseConfiguration, HTableDescriptor}
import org.apache.hadoop.hbase.client.HBaseAdmin
import org.apache.hadoop.hbase.mapreduce.TableInputFormat
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.hbase.HColumnDescriptor
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.client.HTable;
import org.apache.hadoop.hbase.mapred.TableOutputFormat
import org.apache.hadoop.mapred.JobConf
import org.apache.hadoop.hbase.io.ImmutableBytesWritable
import org.apache.hadoop.mapreduce.Job
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat
import org.apache.hadoop.hbase.KeyValue
import org.apache.hadoop.hbase.mapreduce.HFileOutputFormat
import org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles 
import org.apache.commons.lang.StringUtils;
import org.apache.spark.SparkConf;
import org.apache.spark.serializer.KryoRegistrator;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function2;
import com.esotericsoftware.kryo.Kryo;
import com.esotericsoftware.kryo.serializers.FieldSerializer;
object HBaseRead  extends Serializable{
  def main(args: Array[String]) {
    val sparkConf = new SparkConf().setAppName("HBaseRead").setMaster("local[4]")
    val hbaseConfig = HBaseConfiguration.create()
    val tableName = "/datalake/uhclake/prd/p_mtables/mdm_golden_record_repository"
    val table = new HTable(hbaseConfig, tableName)
    hbaseConfig.set("hbase.zookeeper.quorum", "dbslp0304,dbslp0492,dbslp0528");
    hbaseConfig.set("zookeeper.znode.parent","/hbase-unsecure");
    hbaseConfig.set("hbase.zookper.property.clientPort", "2181");
    hbaseConfig.set("fs.defaultFS", "hdfs://dbslp0304:8020");
    hbaseConfig.set(TableInputFormat.INPUT_TABLE, tableName)
	hbaseConfig.set(TableInputFormat.SCAN_COLUMNS, "ri"); 
	hbaseConfig.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer");
	}
	} 
	val hBaseRDD = sc.newAPIHadoopRDD(hbaseConfig, classOf[TableInputFormat],
      classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable],
      classOf[org.apache.hadoop.hbase.client.Result]);
	  val mainr = hBaseRDD.map{item =>
      val immutableBytesWritable = item._2
      Bytes.toString(immutableBytesWritable.get())};
      mainr.take(5).foreach(println);
	  sc.stop()  
	  }
	    }
	  
	  7) hBaseRDD.COUNT();
	  8) hBaseRDD.take(50);
hBaseRDD.toString	  
	  
	  
	  
	  
	  
	  
	  
	  
	  
	  
	  
	  
import org.apache.hadoop.hbase.HBaseConfiguration
import org.apache.hadoop.hbase.client.Put
import org.apache.hadoop.hbase.client.Result
import org.apache.hadoop.hbase.io.ImmutableBytesWritable
import org.apache.hadoop.hbase.mapred.TableOutputFormat
import org.apache.hadoop.hbase.mapreduce.TableInputFormat
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.mapred.JobConf
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.rdd.PairRDDFunctions
import org.apache.spark.rdd.RDD.rddToPairRDDFunctions
import org.apache.spark.util.StatCounter
import org.apache.spark._
import org.apache.spark.rdd.NewHadoopRDD
import org.apache.hadoop.hbase.{HBaseConfiguration, HTableDescriptor}
import org.apache.hadoop.hbase.client.HBaseAdmin
import org.apache.hadoop.hbase.mapreduce.TableInputFormat
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.hbase.HColumnDescriptor
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.client.HTable;
import org.apache.hadoop.hbase.mapred.TableOutputFormat
import org.apache.hadoop.mapred.JobConf
import org.apache.hadoop.hbase.io.ImmutableBytesWritable
import org.apache.hadoop.mapreduce.Job
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat
import org.apache.hadoop.hbase.KeyValue
import org.apache.hadoop.hbase.mapreduce.HFileOutputFormat
import org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles 
import org.apache.commons.lang.StringUtils;
import org.apache.spark.SparkConf;
import org.apache.spark.serializer.KryoRegistrator;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function2;
import com.esotericsoftware.kryo.Kryo;
import com.esotericsoftware.kryo.serializers.FieldSerializer;
object HBaseRead  extends Serializable{
  def main(args: Array[String]) {
    val sparkConf = new SparkConf().setAppName("HBaseRead").setMaster("local[4]")
    val hbaseConfig = HBaseConfiguration.create()
    val tableName = "/datalake/uhclake/prd/p_mtables/mdm_golden_record_repository"
    val table = new HTable(hbaseConfig, tableName)
    hbaseConfig.set("hbase.zookeeper.quorum", "dbslp0304,dbslp0492,dbslp0528");
    hbaseConfig.set("zookeeper.znode.parent","/hbase-unsecure");
    hbaseConfig.set("hbase.zookper.property.clientPort", "2181");
    hbaseConfig.set("fs.defaultFS", "hdfs://dbslp0304:8020");
    hbaseConfig.set(TableInputFormat.INPUT_TABLE, tableName)
    hbaseConfig.set(TableInputFormat.SCAN_COLUMNS, "ri"); 
    hbaseConfig.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer");
hbaseConfig.registerKryoClasses(Array(classOf[org.apache.hadoop.hbase.client.Result]))
    val hBaseRDD = sc.newAPIHadoopRDD(hbaseConfig, classOf[TableInputFormat],
      classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable],
      classOf[org.apache.hadoop.hbase.client.Result]);
	  val mainr = hBaseRDD.map{item =>
      val immutableBytesWritable = item._2
      Bytes.toString(immutableBytesWritable.get())};
      mainr.take(5).foreach(println);
	  sc.stop()  
	  }
	    }
				
import org.apache.hadoop.hbase.util.Bytes
hBaseRDD.map(x=>(Bytes.toStringBinary(x._1.get()))).take(100)
key - org.apache.hadoop.hbase.io.ImmutableBytesWritable
value - org.apache.hadoop.hbase.client.Result


scala> hBaseRDD.map(x=>(x._2.value()))
res40: org.apache.spark.rdd.RDD[Array[Byte]] = MapPartitionsRDD[9] at map at <console>:69
