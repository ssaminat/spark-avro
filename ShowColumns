For getting (show columns in all_tab_columns)
val rdd= sc.textFile("hdfs://nnscbhaasdev/dev/scudee/gps/hdata/gps_dev_ops/gps_all_tab_columns/*");

val rdd1=rdd.map(x=>x.split("\u0002"))

val rdd2 =rdd1.map(x=>(x(0),x(1)))

(TL_STATUS,status_code)
(DS_BANK_REF,versionnum)
(TL_SEPA_SETUP,qlcustid)


val rdd2 =rdd1.map(x=>(x(0),x(1))).foreach(println)

val rdd2=rdd1.filter(x=>x(0).equals("TL_STATUS"))

val rdd2=rdd1.filter(x=>x(0).equals("TL_STATUS")).map(x=>x(1))
rdd2.foreach(println) --------------->status_code
------------------------------------------------------------------
Final:
val rdd= sc.textFile("hdfs://nnscbhaasdev/dev/scudee/gps/hdata/gps_dev_ops/gps_all_tab_columns/*");
val rdd1=rdd.map(x=>x.split("\u0002"))
val rdd2=rdd1.filter(x=>x(0).equals("DS_BANK_REF")).map(x=>x(1))
rdd2.foreach(println)
------------------------------------------------------------------
Equalent Java:
		JavaRDD<String> rdd = sc
				.textFile("C:/Users/1556151.CG-SCB-VISA/Desktop/Important/Project/AVROTEST/testdir/New folder/all_all_tab_columns_V1");
		JavaRDD<String> rdd1 =rdd.map(new Function<String, String[]>(){

			public String[] call(String v1) throws Exception {
				return v1.split("\u0002");
			}}).filter(new Function<String[],Boolean>() {

				public Boolean call(String[] v1) throws Exception {
					return v1[0].equals("TL_STATUS");
				}
			}).map(new Function<String[], String>() {

				public String call(String[] v1) throws Exception {
					return v1[1];
				}
				
			});
		System.out.println("heyyyyy "+rdd1.collect());
-------------------------------------------------------------------------------

Same Using Dataframes as avro:
			DataFrame dataframe = hiveContext
					.read()
					.format("com.databricks.spark.avro")
					.load("C:/Users/1556151.CG-SCB-VISA/Desktop/Important/Project/AVROTEST/testdir/dd/TL_PYMT_DETAILS_CUST_DATA.D2017033.T212723350.R000048.avro");
			String[] columns=dataframe.columns();
			
-------------------------------------------------------------------------------------
val rdd= sc.textFile("/dev/scudee/gps/hdata/gps_dev_ops/gps_all_tab_columns/all_all_tab_columns_V1");
val rdd1=rdd.map(_.split("\u0002"))
val rdd2=rdd1.map((x)=>(x(0),x(1)))
val rdd3=rdd2.groupByKey

JavaSparkContext sc = new JavaSparkContext(prop);
		JavaRDD<String> rdd = sc
				.textFile("/dev/scudee/gps/hdata/gps_dev_ops/gps_all_tab_columns/all_all_tab_columns_V1");
		System.out.println("strasrrere");
		JavaPairRDD<String, Iterable<String>> rdd1=rdd.mapToPair(new PairFunction<String, String, String>() {

			public Tuple2<String, String> call(String v) throws Exception {
				return new Tuple2<String,String>(v.split("\u0002")[0],v.split("\u0002")[1]);
			}
		}).groupByKey().cache();
				Map<String, Iterable<String>> columns =rdd1.collectAsMap();
		
		Iterable<String> ff=columns.get("TL_EOD_MARKER");
---------------------------------------------------------------------------------------------------
